{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94bc9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chitrarthsahai\\Documents\\Repos\\Workbench\n",
      ".venv\n",
      "a2a-python\n",
      "agent-definition-registry-service\n",
      "agent-runtime-service\n",
      "AI Workbench.code-workspace\n",
      "crew\n",
      "DataPreProcessing.ipynb\n",
      "digital-matrix-app\n",
      "digital-matrix-infra\n",
      "digital-matrix-infra.wiki\n",
      "digital-matrix-infra1\n",
      "digital-matrix-microservices\n",
      "digital-matrix-regression\n",
      "digital-matrix-utilities\n",
      "FilesInPy.ipynb\n",
      "InvokeLLMFromTransformers.ipynb\n",
      "Java\n",
      "LangGraphExample.ipynb\n",
      "LangGraphExample_MultiAgent.ipynb\n",
      "OOPsInPy.ipynb\n",
      "semantic-kernel\n",
      "Test.txt\n",
      "wb-core\n"
     ]
    }
   ],
   "source": [
    "#Deepdive into what the python standard library has to offer with respect to file/folder I/O and manipulation.\n",
    "\n",
    "#List a directory in python\n",
    "import os\n",
    "\n",
    "def list_directory(fld):\n",
    "    print(os.getcwd())\n",
    "    for fn in os.listdir(fld):\n",
    "        print(fn)\n",
    "\n",
    "list_directory('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294164e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreProcessing.ipynb\n",
      "FilesInPy.ipynb\n",
      "InvokeLLMFromTransformers.ipynb\n",
      "LangGraphExample.ipynb\n",
      "LangGraphExample_MultiAgent.ipynb\n",
      "OOPsInPy.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#String Methods to look for file name within folder.\n",
    "\n",
    "# def ends_with(fld, search):\n",
    "#     print(os.getcwd())\n",
    "#     for fn in os.listdir(fld):\n",
    "#         if fn.endswith(search):\n",
    "#             print(fn)\n",
    "\n",
    "# def starts_with(fld, search):\n",
    "#     print(os.getcwd())\n",
    "#     for fn in os.listdir(fld):\n",
    "#         if fn.startswith(search):\n",
    "#             print(fn)\n",
    "\n",
    "#lambda implementation of the above\n",
    "\n",
    "ends_with = lambda fld, search: [print(fn) for fn in os.listdir(fld) if fn.endswith(search)]\n",
    "starts_with = lambda fld, search: [print(fn) for fn in os.listdir(fld) if fn.startswith(search)]\n",
    "\n",
    "ends_with('.', '.ipynb')\n",
    "starts_with('.', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c9594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent-definition-registry-service\n",
      "agent-runtime-service\n",
      "digital-matrix-app\n",
      "digital-matrix-infra\n",
      "digital-matrix-infra.wiki\n",
      "digital-matrix-infra1\n",
      "digital-matrix-microservices\n",
      "digital-matrix-regression\n",
      "digital-matrix-utilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pattern matching with fnmatch\n",
    "import fnmatch\n",
    "\n",
    "list_directory_with_pattern = lambda fld, pattern: [print(fn) for fn in os.listdir(fld) if fnmatch.fnmatch(fn, pattern)]\n",
    "\n",
    "list_directory_with_pattern('.', '*[-]*[-]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d758026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent-definition-registry-service\n",
      "agent-runtime-service\n",
      "digital-matrix-app\n",
      "digital-matrix-infra\n",
      "digital-matrix-infra.wiki\n",
      "digital-matrix-infra1\n",
      "digital-matrix-microservices\n",
      "digital-matrix-regression\n",
      "digital-matrix-utilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Advanced pattern matching with fnmatch\n",
    "\n",
    "match = lambda fld, pattern: [print(fn) for fn in os.listdir(fld) if fnmatch.fnmatchcase(fn, pattern)]\n",
    "match('.', '*-*-*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dad132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreProcessing.ipynb\n",
      "FilesInPy.ipynb\n",
      "InvokeLLMFromTransformers.ipynb\n",
      "LangGraphExample.ipynb\n",
      "LangGraphExample_MultiAgent.ipynb\n",
      "OOPsInPy.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pattern matching with glob\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "#This approach is different from the one above, instead of listing contents of the folder and then filtering using fnmatch, we are using the glob module to match the pattern directly.\n",
    "#We are creating a Path object for the given path and then using the glob method to match the pattern. Notice unlike, fnmatch that returns a bool of whether filename is equal to the pattern, glob returns a list of Path objects that match the pattern.\n",
    "glob_match = lambda fld, search: [print(n) for n in Path(fld).glob(search)]\n",
    "\n",
    "glob_match('.', '*.ipynb')\n",
    "\n",
    "#You avoid iterating over the entire directory and then filtering it. Instead, you are directly getting the list of files that match the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11d12c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chitrarthsahai\\Documents\\Repos\\Workbench\n",
      ".venv is a directory\n",
      "a2a-python is a directory\n",
      "agent-definition-registry-service is a directory\n",
      "agent-runtime-service is a directory\n",
      "Modified 04 May 2025 AI Workbench.code-workspace 76 bytes\n",
      "crew is a directory\n",
      "Modified 04 May 2025 DataPreProcessing.ipynb 255 bytes\n",
      "digital-matrix-app is a directory\n",
      "digital-matrix-infra is a directory\n",
      "digital-matrix-infra.wiki is a directory\n",
      "digital-matrix-infra1 is a directory\n",
      "digital-matrix-microservices is a directory\n",
      "digital-matrix-regression is a directory\n",
      "digital-matrix-utilities is a directory\n",
      "Modified 21 May 2025 FilesInPy.ipynb 8986 bytes\n",
      "Modified 04 May 2025 InvokeLLMFromTransformers.ipynb 9018 bytes\n",
      "Java is a directory\n",
      "Modified 17 May 2025 LangGraphExample.ipynb 15617 bytes\n",
      "Modified 17 May 2025 LangGraphExample_MultiAgent.ipynb 76988 bytes\n",
      "Modified 20 May 2025 OOPsInPy.ipynb 493737 bytes\n",
      "semantic-kernel is a directory\n",
      "wb-core is a directory\n"
     ]
    }
   ],
   "source": [
    "#Get File Attributes.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "get_date = lambda timestamp: datetime.fromtimestamp(timestamp).strftime('%d %b %Y')\n",
    "\n",
    "def get_file_attributes(fld):\n",
    "    print(os.getcwd())\n",
    "    with os.scandir(fld) as dir:\n",
    "        for f in dir:\n",
    "            if f.is_file():\n",
    "                inf = f.stat()\n",
    "                print(f'Modified {get_date(inf.st_mtime)} {f.name} {inf.st_size} bytes')\n",
    "            elif f.is_dir():\n",
    "                print(f.name, 'is a directory')\n",
    "            else:\n",
    "                print(f.name, 'is not a file or directory')\n",
    "\n",
    "get_file_attributes('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traversing a directory tree\n",
    "\n",
    "def traverse_directory(fld):\n",
    "    print(os.getcwd())\n",
    "    for dirpath, dirnames, filenames in os.walk(fld):\n",
    "        print(f'Current directory: {dirpath}')\n",
    "        print(f'Directories: {dirnames}')\n",
    "        print(f'Files: {filenames}')\n",
    "        print('---')\n",
    "\n",
    "traverse_directory('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d28701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying files\n",
    "\n",
    "import shutil\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    print(os.getcwd())\n",
    "    if os.path.isfile(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f'Copied {src} to {dst}')\n",
    "    else:\n",
    "        print(f'{src} does not exist')\n",
    "\n",
    "def copy_directory(src, dst):\n",
    "    print(os.getcwd())\n",
    "    if os.path.isdir(src):\n",
    "        shutil.copytree(src, dst)\n",
    "        print(f'Copied {src} to {dst}')\n",
    "    else:\n",
    "        print(f'{src} does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd44a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_file(src, dst): #can move both files and directories\n",
    "    #shutil.move(src, dst) #this will move the file or directory to the destination\n",
    "    print(os.getcwd())\n",
    "    if os.path.isfile(src):\n",
    "        shutil.move(src, dst)\n",
    "        print(f'Copied {src} to {dst}')\n",
    "    else:\n",
    "        print(f'{src} does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55b1f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename files\n",
    "\n",
    "def rename_file(src, dst):\n",
    "    os.rename(src, dst) #one way to rename\n",
    "\n",
    "def rename_file_2(src, dst):\n",
    "    Path(src).rename(dst) #another way to rename\n",
    "\n",
    "#rename_file('LangGraphExample.ipynb', 'LangGraph_Example.ipynb')\n",
    "rename_file_2('LangGraph_Example.ipynb', 'LangGraphExample.ipynb')\n",
    "#Deleting files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e26ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ./Test.txt\n"
     ]
    }
   ],
   "source": [
    "#deleting files and directories\n",
    "def delete_file(src):\n",
    "    if os.path.isfile(src):\n",
    "        try:\n",
    "            os.remove(src)\n",
    "            print(f'Deleted {src}')\n",
    "        except OSError as e:\n",
    "            print(f'Error deleting {src}: {e}')\n",
    "    else:\n",
    "            print(f'{src} does not exist')\n",
    "\n",
    "delete_file(\"./Test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a983e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added test.txt to test_1.zip\n",
      "Added test copy.txt to test_1.zip\n",
      "Added test copy 2.txt to test_1.zip\n",
      "Added test copy 3.txt to test_1.zip\n",
      "Added test copy 4.txt to test_1.zip\n"
     ]
    }
   ],
   "source": [
    "#Archiving files and directories\n",
    "import zipfile\n",
    "\n",
    "def create_zip(zipf:str, files:list[str], opt):\n",
    "    with zipfile.ZipFile(zipf, opt) as zf:\n",
    "        for f in files:\n",
    "            zf.write(f)\n",
    "            print(f'Added {f} to {zipf}')\n",
    "\n",
    "to_zip = [\n",
    "    'test.txt',\n",
    "    \"test copy.txt\",\n",
    "    \"test copy 2.txt\",\n",
    "    \"test copy 3.txt\",\n",
    "    \"test copy 4.txt\",\n",
    "]\n",
    "\n",
    "create_zip('test_1.zip', to_zip, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00fb9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.txt already exists in test_1.zip\n",
      "test copy.txt already exists in test_1.zip\n",
      "test copy 2.txt already exists in test_1.zip\n",
      "test copy 3.txt already exists in test_1.zip\n",
      "test copy 4.txt already exists in test_1.zip\n",
      "Added test copy 5.txt to test_1.zip\n",
      "Added test copy 6.txt to test_1.zip\n",
      "Added test copy 7.txt to test_1.zip\n"
     ]
    }
   ],
   "source": [
    "#adding files to an existing zip file\n",
    "\n",
    "def add_to_zip(zipf:str, files:list[str], opt):\n",
    "    with zipfile.ZipFile(zipf, opt) as zf:\n",
    "        for f in files:\n",
    "            lst = zf.namelist()\n",
    "            if f not in lst:\n",
    "                zf.write(f)\n",
    "                print(f'Added {f} to {zipf}')\n",
    "            else:\n",
    "                print(f'{f} already exists in {zipf}')\n",
    "\n",
    "to_zip = [\n",
    "    'test.txt',\n",
    "    \"test copy.txt\",\n",
    "    \"test copy 2.txt\",\n",
    "    \"test copy 3.txt\",\n",
    "    \"test copy 4.txt\",\n",
    "    \"test copy 5.txt\",\n",
    "    \"test copy 6.txt\",\n",
    "    \"test copy 7.txt\"\n",
    "]\n",
    "\n",
    "add_to_zip('test_1.zip', to_zip, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e674373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info of test.txt: <ZipInfo filename='test.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy.txt: <ZipInfo filename='test copy.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 2.txt: <ZipInfo filename='test copy 2.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 2.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 3.txt: <ZipInfo filename='test copy 3.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 3.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 4.txt: <ZipInfo filename='test copy 4.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 4.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test.txt: <ZipInfo filename='test.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy.txt: <ZipInfo filename='test copy.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 2.txt: <ZipInfo filename='test copy 2.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 2.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 3.txt: <ZipInfo filename='test copy 3.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 3.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 4.txt: <ZipInfo filename='test copy 4.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 4.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 5.txt: <ZipInfo filename='test copy 5.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 5.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 6.txt: <ZipInfo filename='test copy 6.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 6.txt => 0 bytes, 0 bytes compressed\n",
      "Info of test copy 7.txt: <ZipInfo filename='test copy 7.txt' filemode='-rw-rw-rw-' file_size=0>\n",
      "test copy 7.txt => 0 bytes, 0 bytes compressed\n"
     ]
    }
   ],
   "source": [
    "#Reading a zip file\n",
    "\n",
    "def read_zip(zipf:str):\n",
    "    with zipfile.ZipFile(zipf, 'r') as zf:\n",
    "        for f in zf.namelist():\n",
    "            print(f'Info of {f}: {zf.getinfo(f)}')\n",
    "            print(f'{f} => {zf.getinfo(f).file_size} bytes, {zf.getinfo(f).compress_size} bytes compressed')\n",
    "\n",
    "read_zip('test_1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b3a746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted test_1.zip to ./extracted\n"
     ]
    }
   ],
   "source": [
    "#Extracting files from a zip file\n",
    "\n",
    "def extract_zip(zipf:str, dst:str):\n",
    "    with zipfile.ZipFile(zipf, 'r') as zf:\n",
    "        zf.extractall(dst)\n",
    "        print(f'Extracted {zipf} to {dst}')\n",
    "\n",
    "extract_zip('test_1.zip', './extracted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad266639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{---------------------------------------\n",
      "\"cells\": [---------------------------------------\n",
      "{---------------------------------------\n",
      "\"cell_type\": \"code\",---------------------------------------\n",
      "\"execution_count\": 2,---------------------------------------\n",
      "\"id\": \"87e462aa\",---------------------------------------\n",
      "\"metadata\": {},---------------------------------------\n",
      "\"outputs\": [],---------------------------------------\n",
      "\"source\": [---------------------------------------\n",
      "\"%%capture --no-stderr\\n\",---------------------------------------\n",
      "\"%pip install --quiet transformers torch --verbose\"---------------------------------------\n",
      "]---------------------------------------\n",
      "},---------------------------------------\n",
      "{---------------------------------------\n",
      "\"cell_type\": \"code\",---------------------------------------\n",
      "\"execution_count\": null,---------------------------------------\n",
      "\"id\": \"625fcaa7\",---------------------------------------\n",
      "\"metadata\": {},---------------------------------------\n",
      "\"outputs\": [---------------------------------------\n",
      "{---------------------------------------\n",
      "\"name\": \"stdout\",---------------------------------------\n",
      "\"output_type\": \"stream\",---------------------------------------\n",
      "\"text\": [---------------------------------------\n",
      "\"Using device: cpu\\n\"---------------------------------------\n",
      "]---------------------------------------\n",
      "},---------------------------------------\n",
      "{---------------------------------------\n",
      "\"ename\": \"OSError\",---------------------------------------\n",
      "\"evalue\": \"Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.\",---------------------------------------\n",
      "\"output_type\": \"error\",---------------------------------------\n",
      "\"traceback\": [---------------------------------------\n",
      "\"\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\",---------------------------------------\n",
      "\"\\u001b[1;31mOSError\\u001b[0m                                   Traceback (most recent call last)\",---------------------------------------\n",
      "\"Cell \\u001b[1;32mIn[3], line 5\\u001b[0m\\n\\u001b[0;32m      3\\u001b[0m device \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mcuda\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m \\u001b[38;5;28;01mif\\u001b[39;00m torch\\u001b[38;5;241m.\\u001b[39mcuda\\u001b[38;5;241m.\\u001b[39mis_available() \\u001b[38;5;28;01melse\\u001b[39;00m \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mcpu\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m      4\\u001b[0m \\u001b[38;5;28mprint\\u001b[39m(\\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mUsing device: \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mdevice\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m)\\n\\u001b[1;32m----> 5\\u001b[0m tokenizer \\u001b[38;5;241m=\\u001b[39m \\u001b[43mAutoTokenizer\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mfrom_pretrained\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\\u001b[39;49m\\u001b[38;5;124;43m\\\"\\u001b[39;49m\\u001b[43m)\\u001b[49m\\n\\u001b[0;32m      6\\u001b[0m model \\u001b[38;5;241m=\\u001b[39m transformers\\u001b[38;5;241m.\\u001b[39mAutoModelForCausalLM\\u001b[38;5;241m.\\u001b[39mfrom_pretrained(\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, device_map\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mauto\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m, torch_dtype\\u001b[38;5;241m=\\u001b[39mtorch\\u001b[38;5;241m.\\u001b[39mfloat16)\\n\\u001b[0;32m      8\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m\\u001b[38;5;250m \\u001b[39m\\u001b[38;5;21mgenerate_text\\u001b[39m(prompt, max_length\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;241m50\\u001b[39m):\\n\",---------------------------------------\n",
      "\"File \\u001b[1;32mc:\\\\Users\\\\chitrarthsahai\\\\Documents\\\\Repos\\\\Workbench\\\\.venv\\\\lib\\\\site-packages\\\\transformers\\\\models\\\\auto\\\\tokenization_auto.py:1028\\u001b[0m, in \\u001b[0;36mAutoTokenizer.from_pretrained\\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\\u001b[0m\\n\\u001b[0;32m   1025\\u001b[0m tokenizer_class_py, tokenizer_class_fast \\u001b[38;5;241m=\\u001b[39m TOKENIZER_MAPPING[\\u001b[38;5;28mtype\\u001b[39m(config)]\\n\\u001b[0;32m   1027\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m tokenizer_class_fast \\u001b[38;5;129;01mand\\u001b[39;00m (use_fast \\u001b[38;5;129;01mor\\u001b[39;00m tokenizer_class_py \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m):\\n\\u001b[1;32m-> 1028\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m tokenizer_class_fast\\u001b[38;5;241m.\\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \\u001b[38;5;241m*\\u001b[39minputs, \\u001b[38;5;241m*\\u001b[39m\\u001b[38;5;241m*\\u001b[39mkwargs)\\n\\u001b[0;32m   1029\\u001b[0m \\u001b[38;5;28;01melse\\u001b[39;00m:\\n\\u001b[0;32m   1030\\u001b[0m     \\u001b[38;5;28;01mif\\u001b[39;00m tokenizer_class_py \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m:\\n\",---------------------------------------\n",
      "\"File \\u001b[1;32mc:\\\\Users\\\\chitrarthsahai\\\\Documents\\\\Repos\\\\Workbench\\\\.venv\\\\lib\\\\site-packages\\\\transformers\\\\tokenization_utils_base.py:2046\\u001b[0m, in \\u001b[0;36mPreTrainedTokenizerBase.from_pretrained\\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\\u001b[0m\\n\\u001b[0;32m   2043\\u001b[0m \\u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\\u001b[39;00m\\n\\u001b[0;32m   2044\\u001b[0m \\u001b[38;5;66;03m# loaded directly from the GGUF file.\\u001b[39;00m\\n\\u001b[0;32m   2045\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;28mall\\u001b[39m(full_file_name \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m \\u001b[38;5;28;01mfor\\u001b[39;00m full_file_name \\u001b[38;5;129;01min\\u001b[39;00m resolved_vocab_files\\u001b[38;5;241m.\\u001b[39mvalues()) \\u001b[38;5;129;01mand\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m gguf_file:\\n\\u001b[1;32m-> 2046\\u001b[0m     \\u001b[38;5;28;01mraise\\u001b[39;00m \\u001b[38;5;167;01mEnvironmentError\\u001b[39;00m(\\n\\u001b[0;32m   2047\\u001b[0m         \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mCan\\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mt load tokenizer for \\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mpretrained_model_name_or_path\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124m. If you were trying to load it from \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m   2048\\u001b[0m         \\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mhttps://huggingface.co/models\\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124m, make sure you don\\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mt have a local directory with the same name. \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m   2049\\u001b[0m         \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mOtherwise, make sure \\u001b[39m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mpretrained_model_name_or_path\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124m is the correct path to a directory \\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m   2050\\u001b[0m         \\u001b[38;5;124mf\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mcontaining all relevant files for a \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00m\\u001b[38;5;28mcls\\u001b[39m\\u001b[38;5;241m.\\u001b[39m\\u001b[38;5;18m__name__\\u001b[39m\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[38;5;124m tokenizer.\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m\\n\\u001b[0;32m   2051\\u001b[0m     )\\n\\u001b[0;32m   2053\\u001b[0m \\u001b[38;5;28;01mfor\\u001b[39;00m file_id, file_path \\u001b[38;5;129;01min\\u001b[39;00m vocab_files\\u001b[38;5;241m.\\u001b[39mitems():\\n\\u001b[0;32m   2054\\u001b[0m     \\u001b[38;5;28;01mif\\u001b[39;00m file_id \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;129;01min\\u001b[39;00m resolved_vocab_files:\\n\",---------------------------------------\n",
      "\"\\u001b[1;31mOSError\\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.\"---------------------------------------\n",
      "]---------------------------------------\n",
      "}---------------------------------------\n",
      "],---------------------------------------\n",
      "\"source\": [---------------------------------------\n",
      "\"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",---------------------------------------\n",
      "\"import torch\\n\",---------------------------------------\n",
      "\"device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\",---------------------------------------\n",
      "\"print(f\\\"Using device: {device}\\\")\\n\",---------------------------------------\n",
      "\"tokenizer = AutoTokenizer.from_pretrained(\\\"meta-llama/Llama-2-7b-chat-hf\\\")\\n\",---------------------------------------\n",
      "\"model = AutoModelForCausalLM.from_pretrained(\\\"meta-llama/Llama-2-7b-chat-hf\\\", device_map=\\\"auto\\\", torch_dtype=torch.float16)\\n\",---------------------------------------\n",
      "\"\\n\",---------------------------------------\n",
      "\"def generate_text(prompt, max_length=50):\\n\",---------------------------------------\n",
      "\"    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(device)\\n\",---------------------------------------\n",
      "\"    outputs = model.generate(**inputs, max_length=max_length)\\n\",---------------------------------------\n",
      "\"    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\",---------------------------------------\n",
      "\"\\n\",---------------------------------------\n",
      "\"#Start an interactive chat\\n\",---------------------------------------\n",
      "\"\\n\",---------------------------------------\n",
      "\"while True:\\n\",---------------------------------------\n",
      "\"    user_input = input(\\\"You: \\\")\\n\",---------------------------------------\n",
      "\"    if user_input.lower() == \\\"exit\\\":\\n\",---------------------------------------\n",
      "\"        break\\n\",---------------------------------------\n",
      "\"    response = generate_text(user_input, max_length=100)\\n\",---------------------------------------\n",
      "\"    print(f\\\"Model: {response}\\\")\"---------------------------------------\n",
      "]---------------------------------------\n",
      "}---------------------------------------\n",
      "],---------------------------------------\n",
      "\"metadata\": {---------------------------------------\n",
      "\"kernelspec\": {---------------------------------------\n",
      "\"display_name\": \".venv\",---------------------------------------\n",
      "\"language\": \"python\",---------------------------------------\n",
      "\"name\": \"python3\"---------------------------------------\n",
      "},---------------------------------------\n",
      "\"language_info\": {---------------------------------------\n",
      "\"codemirror_mode\": {---------------------------------------\n",
      "\"name\": \"ipython\",---------------------------------------\n",
      "\"version\": 3---------------------------------------\n",
      "},---------------------------------------\n",
      "\"file_extension\": \".py\",---------------------------------------\n",
      "\"mimetype\": \"text/x-python\",---------------------------------------\n",
      "\"name\": \"python\",---------------------------------------\n",
      "\"nbconvert_exporter\": \"python\",---------------------------------------\n",
      "\"pygments_lexer\": \"ipython3\",---------------------------------------\n",
      "\"version\": \"3.10.4\"---------------------------------------\n",
      "}---------------------------------------\n",
      "},---------------------------------------\n",
      "\"nbformat\": 4,---------------------------------------\n",
      "\"nbformat_minor\": 5---------------------------------------\n",
      "}---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#working with text files\n",
    "\n",
    "def read_text(fn):\n",
    "    with open(fn) as f:\n",
    "        print(f.read())\n",
    "\n",
    "\n",
    "def read_text_line(fn):\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            print(line.strip(), end='')\n",
    "            print('---------------------------------------')\n",
    "\n",
    "\n",
    "def write_new_text(fn, text):\n",
    "    with open(fn, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "        print(f'Wrote {text} to {fn}')\n",
    "\n",
    "def append_text(fn, text):\n",
    "    with open(fn, 'a', encoding='utf-8') as f:\n",
    "        f.write('\\n')\n",
    "        f.write(text)\n",
    "        print(f'Appended {text} to {fn}')\n",
    "\n",
    "# read_text('InvokeLLMFromTransformers.ipynb')\n",
    "# read_text_line('InvokeLLMFromTransformers.ipynb')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecc17e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubnetName | CIDRRange | ShortDescription\n",
      "sub-core-pe-wb-gac-use-ua | 10.56.6.0/25 | Core PE subnet\n",
      "AzureFirewallSubnet | 10.56.0.0/26 | Firewall subnet\n",
      "sub-dpr-outbound-wb-gac-use-ua | 10.56.1.16/28 | Outbound DPR subnet\n",
      "sub-dpr-inbound-wb-gac-use-ua | 10.56.1.32/28 | Inbound DPR subnet\n",
      "sub-core-buildagent-wb-gac-use-ua | 10.56.0.128/25 | Build agent subnet\n",
      "AzureBastionSubnet | 10.56.0.64/26 | Bastion service subnet\n",
      "sub-pe-wb-gac-use-ua | 10.56.5.0/25 | PE subnet\n",
      "sub-asp-wb-gac-use-ua | 10.56.6.192/26 | ASP subnet\n",
      "sub-apim-wb-gac-use-ua | 10.56.5.192/26 | APIM subnet\n",
      "Total rows: 9\n",
      "Wrote ['Name', 'Age', 'City'] and ['John', '30', 'New York'] to test.csv\n"
     ]
    }
   ],
   "source": [
    "#Working with CSV files\n",
    "import csv\n",
    "from typing import Iterable, Any\n",
    "\n",
    "def read_csv(fn, delimiter):\n",
    "    with open(fn) as csv_f:\n",
    "        cnt = -1\n",
    "        rows = csv.reader(csv_f,delimiter=delimiter)\n",
    "        for row in rows:\n",
    "                print(f'{\" | \".join(row)}')\n",
    "                cnt += 1\n",
    "        print(f'Total rows: {cnt}')\n",
    "\n",
    "read_csv('SubnetDetails.csv', ',')\n",
    "\n",
    "\n",
    "def write_csv(fn, header:Iterable[Any], row:Iterable[Any]):\n",
    "     with open(fn, mode='w', newline='') as csv_f:\n",
    "        writer = csv.writer(csv_f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerow(row)\n",
    "        print(f'Wrote {header} and {row} to {fn}')\n",
    "\n",
    "header = ['Name', 'Age', 'City']\n",
    "row = ['John', '30', 'New York']\n",
    "write_csv('test.csv', header, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0de3e8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domains for:data\n",
      "data => [] having values \n",
      "    \n",
      "country => ['name:Liechtenstein'] having values \n",
      "        \n",
      "rank => [] having values 1\n",
      "year => [] having values 2008\n",
      "gdppc => [] having values 141100\n",
      "neighbor => ['name:Austria', 'direction:E'] having values None\n",
      "neighbor => ['name:Switzerland', 'direction:W'] having values None\n",
      "country => ['name:Singapore'] having values \n",
      "        \n",
      "rank => [] having values 4\n",
      "year => [] having values 2011\n",
      "gdppc => [] having values 59900\n",
      "neighbor => ['name:Malaysia', 'direction:N'] having values None\n",
      "country => ['name:Panama'] having values \n",
      "        \n",
      "rank => [] having values 68\n",
      "year => [] having values 2011\n",
      "gdppc => [] having values 13600\n",
      "neighbor => ['name:Costa Rica', 'direction:W'] having values None\n",
      "neighbor => ['name:Colombia', 'direction:E'] having values None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     root.find(el).set(attr, val) \u001b[38;5;66;03m#changing the value of the attribute\u001b[39;00m\n\u001b[32m     27\u001b[39m     tree.write(fn)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mchange_xml_er\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample.xml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneighbor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexample.org\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mchange_xml_er\u001b[39m\u001b[34m(fn, el, attr, val)\u001b[39m\n\u001b[32m     24\u001b[39m tree = ET.parse(fn)\n\u001b[32m     25\u001b[39m root = tree.getroot()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mroot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m(attr, val) \u001b[38;5;66;03m#changing the value of the attribute\u001b[39;00m\n\u001b[32m     27\u001b[39m tree.write(fn)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'set'"
     ]
    }
   ],
   "source": [
    "#Working with XML files\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml_et(fn):\n",
    "    tree = ET.parse(fn)\n",
    "    root = tree.getroot()\n",
    "    print('Domains for:' + root.tag)\n",
    "    for items in root.iter():\n",
    "        print(f'{items.tag} => {[f'{k}:{v}' for k,v in items.attrib.items()]} having values {items.text}')\n",
    "\n",
    "parse_xml_et('sample.xml')\n",
    "\n",
    "def add_xml_et(fn, el, attr, val):\n",
    "    tree = ET.parse(fn)\n",
    "    root = tree.getroot()\n",
    "    new_el = ET.Element(el, attr) #instantiating a new element\n",
    "    new_el.text = val\n",
    "    root.append(new_el)\n",
    "    tree.write(fn)\n",
    "    print(f'Added {el} with attributes {attr} and value {val} to {fn}')\n",
    "\n",
    "def change_xml_er(fn, el, attr, val):\n",
    "    tree = ET.parse(fn)\n",
    "    root = tree.getroot()\n",
    "    root.find(el).set(attr, val) #changing the value of the attribute\n",
    "    tree.write(fn)\n",
    "\n",
    "change_xml_er('sample.xml', 'neighbor', 'name', 'example.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bb615c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"authors\": [\n",
      "        {\n",
      "            \"courses\": 10,\n",
      "            \"name\": \"John Doe\"\n",
      "        },\n",
      "        {\n",
      "            \"courses\": 10,\n",
      "            \"name\": \"Jane Smith\"\n",
      "        },\n",
      "        {\n",
      "            \"courses\": 5,\n",
      "            \"name\": \"Foo Fighter\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Updated name to Mark Twain in authors at position 0 in authors.json\n"
     ]
    }
   ],
   "source": [
    "#Working with JSON files\n",
    "\n",
    "import json\n",
    "\n",
    "def read_print_json(fn, pretty, sort):\n",
    "    with open(fn) as json_f:\n",
    "        data = json.load(json_f)\n",
    "        if pretty:\n",
    "            print(json.dumps(data, indent=4, sort_keys=sort))\n",
    "        else:\n",
    "            print(json.dumps(data))\n",
    "\n",
    "def update_author_json(fn, arr_name, pos, key, value):\n",
    "       with open(fn) as json_f:\n",
    "            data = json.load(json_f)\n",
    "            data[arr_name][pos][key] = value\n",
    "            with open(fn, 'w') as json_f:\n",
    "                json.dump(data, json_f, indent=4)\n",
    "                print(f'Updated {key} to {value} in {arr_name} at position {pos} in {fn}')\n",
    "\n",
    "read_print_json('authors.json', True, True)\n",
    "\n",
    "update_author_json('authors.json', 'authors', 0, 'name', 'Mark Twain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80f0e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled object: b'\\x80\\x05\\x95\\x1a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\\x94\\x8c\\x06Person\\x94\\x93\\x94)\\x81\\x94.'\n",
      "Unpickled object: <__main__.Person object at 0x0000022A3C923B10>\n"
     ]
    }
   ],
   "source": [
    "#Persisting data with pickle\n",
    "\n",
    "import pickle as p\n",
    "\n",
    "\n",
    "class Person:\n",
    "    age = 45\n",
    "    name = 'John Doe'\n",
    "    kids = ['Pete', 'Mary', 'Tom']\n",
    "    employers = {'AWS': 2022, 'Google': 2023, 'Microsoft': 2024}\n",
    "    shoe_sizes = (10, 11, 12)\n",
    "\n",
    "person = Person()\n",
    "\n",
    "def serialize(obj):\n",
    "    pickled = p.dumps(obj, protocol=p.HIGHEST_PROTOCOL)\n",
    "    print(f'Pickled object: {pickled}')\n",
    "    return pickled\n",
    "\n",
    "def deserialize(pickled):\n",
    "    obj = p.loads(pickled)\n",
    "    print(f'Unpickled object: {obj}')\n",
    "    return obj\n",
    "\n",
    "serialized = serialize(person)\n",
    "\n",
    "deserialized = deserialize(serialized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccd6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
